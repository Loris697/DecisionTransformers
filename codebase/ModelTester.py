import os
import torch
import torch.nn as nn
import gymnasium as gym
from gymnasium import spaces
import numpy as np
import imageio
from typing import Callable

class ModelTester:
    def __init__(self, 
                 env_name: str,
                 device: str = "cuda",
                 seq_len: int = 32, 
                 render_mode: str = "human", 
                 actionCheck: Callable = None, 
                 max_reward: float = 800.
                ):
        """
        Initialize the tester with a specific environment and a model.

        Args:
            env_name (str): The name of the environment to create with Gym.
            model (nn.Module): The model to test.
            device (str): Device to run the model on, 'cuda' or 'cpu'.
            seq_len (int): Length of the sequence for the model to process.
            render_mode (str): Mode to render the environment, e.g., 'human' or 'rgb_array'.
            actionCheck (Callable, optional): A function to process or validate actions generated by the model.
            max_reward (float): Scaling factor for rewards during training/testing.
        """
        self.env = gym.make(env_name, render_mode=render_mode)
        self.device = device
        self.seq_len = seq_len
        self.actionCheck = actionCheck
        self.max_reward = max_reward
        self.render_mode = render_mode

        # Initialize storage tensors for rewards, observations, and actions
        self.rewards = torch.zeros(1, seq_len, 1)
        obs_shape = self.env.observation_space.shape
        self.observations = torch.zeros(1, seq_len, obs_shape[2], obs_shape[0], obs_shape[1])
        self.actions = torch.zeros(1, seq_len, self.env.action_space.shape[0])

    def run_episode(self, model, render=False, starting_reward=1.0, file_name="episode_output", 
                    starting_action=None):
        """
        Run one episode using the model in the environment.

        Args:
            render (bool): Whether to render the episode.
            starting_reward (float): Initial reward to start the episode.
            file_name (str): Base file name to save output data, such as videos or GIFs.
            starting_action (gym.spaces.Space, optional): The initial action to start the episode.

        Returns:
            float: The total reward accumulated during the episode.
        """
        state, _ = self.env.reset()
        done = False
        truncated = False
        total_reward = 0
        step = 0
        model = model.to(device=self.device)
        starting_action = starting_action or self.env.action_space.sample()

        self.reset_sequence()
        self.rewards[0, step] = starting_reward
        self.actions[0, step] = torch.tensor(starting_action, dtype=torch.float32)
        self.observations[0, step] = torch.tensor(state / 255.0).permute(2, 0, 1)

        frames = []  # Store frames if rendering to RGB array

        while not (done or truncated):
            if render and self.render_mode in ["human", "rgb_array"]:
                frame = self.env.render()
                if self.render_mode == "rgb_array":
                    frames.append(frame)

            model_input = {
                "rewards": self.rewards.to(self.device, dtype=torch.float32),
                "observations": self.observations.to(self.device, dtype=torch.float32),
                "actions": self.actions.to(self.device, dtype=torch.float32),
            }
            actions = model(model_input)
            actions = actions.cpu().detach().numpy()

            if self.actionCheck:
                action = self.actionCheck(actions[0][step])

            state, reward, done, truncated, info = self.env.step(action)
            total_reward += reward

            # Update sequence tensors
            next_reward = (total_reward / self.max_reward) - reward
            step = min(step + 1, self.seq_len - 1)
            self.rewards[0, step] = next_reward
            self.actions[0, step] = torch.tensor(action, dtype=torch.float32)
            self.observations[0, step] = torch.tensor( state / 255.0).permute(2, 0, 1)

        if render and self.render_mode == "rgb_array":
            gif_path = f"{file_name}.gif"
            imageio.mimsave(gif_path, frames, fps=30)
            print(f"Saved episode gif to {gif_path}")

        return total_reward

    def test_model(self, 
                   model,
                   episodes=100, 
                   render=False, 
                   starting_rewards=[1.0], 
                   folder="output_folder"
                  ):
        """
        Test the model over a specified number of episodes and log the results.

        Args:
            episodes (int): Number of episodes to run for testing.
            render (bool): Whether to render episodes.
            starting_rewards (list): List of starting rewards to test.
            folder (str): Directory to save any output files.

        Returns:
            dict: A dictionary mapping starting rewards to average rewards.
        """
        average_reward_starting = {}
        if not os.path.exists(folder):
            os.makedirs(folder)

        for starting_reward in starting_rewards:
            total_rewards = [self.run_episode(
                                              model = model,
                                              render=render,
                                              starting_reward=starting_reward,
                                              file_name=os.path.join(folder, f"starting_rewards{starting_reward}_{i}"))
                             for i in range(episodes)]
            average_reward = sum(total_rewards) / episodes
            average_reward_starting[str(starting_reward)] = average_reward

        return average_reward_starting

    def close_env(self):
        """Close the Gym environment to free resources."""
        self.env.close()

    def reset_sequence(self):
        """Reset the tensors storing sequences of rewards, observations, and actions."""
        self.rewards.fill_(0)
        self.observations.fill_(0)
        self.actions.fill_(0)