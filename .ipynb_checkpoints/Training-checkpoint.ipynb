{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4761f9f-4572-496a-9a70-9b9d10da06f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 11:01:43.879613: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import gymnasium as gym\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d983f1-434d-4817-b8a6-fa7f5061ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'CarRacing-v2'\n",
    "render_mode = \"rgb_array\"\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make(env_id, render_mode=render_mode)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "debda858-93ec-4952-ac7e-4a2e0c4061fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_out_conv_layer(in_h, in_w, ker, pad = 0, dil = 1, stri = 1):\n",
    "    out_h = in_h\n",
    "    out_w = in_w\n",
    "    \n",
    "    out_h = (out_h + 2*pad - dil * (ker-1) - 1)//stri + 1\n",
    "    out_w = (out_w + 2*pad - dil * (ker-1) - 1)//stri + 1\n",
    "\n",
    "    return out_h, out_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72211b00-4ee5-4e9d-9ccc-84ab7baaa30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, observation_space, features_dim: int = 256, \n",
    "                 hidden_channels: int = 32, n_cnn_layers: int = 3, \n",
    "                 stride: int = 1, dropout : float = 0.2):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.dropout =  dropout   \n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "        n_input_channels = observation_space.shape[2]\n",
    "        image_h = observation_space.shape[0]\n",
    "        image_w = observation_space.shape[1]\n",
    "        out_h_first_2, out_w_first_2 = calc_out_conv_layer(image_h, image_w, n_input_channels, stri = stride)\n",
    "        #Now the size is hidden_channels x out_h x out_w\n",
    "        out_h_first = out_h_first_2 // 2\n",
    "        out_w_first = out_w_first_2 // 2\n",
    "        \n",
    "        self.first_cnn_layer = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, hidden_channels, kernel_size=3, stride=stride),\n",
    "            nn.LayerNorm([hidden_channels, out_h_first_2, out_w_first_2]),  # Add Layer Normalization\n",
    "            nn.GELU(),\n",
    "            nn.Dropout2d(p=self.dropout),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        ##Now the size is hidden_channels x out_h_first x out_w_first\n",
    "        out_h_2, out_w_2 = calc_out_conv_layer(out_h_first, out_w_first, 3, stri = stride)\n",
    "        \n",
    "        self.second_cnn_layer = nn.Sequential(\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, stride=stride),\n",
    "            nn.LayerNorm([hidden_channels, out_h_2, out_w_2]),  # Add Layer Normalization\n",
    "            nn.GELU(),\n",
    "            nn.Dropout2d(p=self.dropout),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        #Now the size is hidden_channels x out_h x out_w\n",
    "        out_h = out_h_2 // 2\n",
    "        out_w = out_w_2 // 2\n",
    "        \n",
    "        # Define a list to hold the CNN layers\n",
    "        self.cnn_layers = nn.ModuleList()\n",
    "        \n",
    "        # Add the specified number of convolutional layers\n",
    "        for index in range(n_cnn_layers - 2):\n",
    "            modules = []\n",
    "            modules.append(nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, stride=stride, padding=\"same\"))\n",
    "            modules.append(nn.LayerNorm([hidden_channels, out_h, out_w]))  # Add Layer Normalization\n",
    "            modules.append(nn.GELU())  # Use GELU activation function after LN\n",
    "            #modules.append(nn.MaxPool2d(kernel_size=2, stride=2))  # Add MaxPooling layer\n",
    "            modules.append(nn.Dropout2d(p=0.2))  # Add Dropout (DropBlock) layer\n",
    "            self.cnn_layers.append(nn.Sequential(*modules))\n",
    "\n",
    "        # Define the Flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            # Define a dummy input tensor\n",
    "            dummy_input = torch.as_tensor(observation_space.sample()[None]).permute(0, 3, 1, 2).float()\n",
    "            # Perform a forward pass through the CNN layers\n",
    "            cnn_output = dummy_input\n",
    "            cnn_output = self.first_cnn_layer(cnn_output)\n",
    "            cnn_output = self.second_cnn_layer(cnn_output)\n",
    "            for layer in self.cnn_layers:\n",
    "                cnn_output = cnn_output + layer(cnn_output)\n",
    "            # Compute the shape after flattening\n",
    "            n_flatten = self.flatten(cnn_output).shape[1]\n",
    "\n",
    "        # Define the linear layer\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.GELU())\n",
    "        # Print the number of learnable parameters\n",
    "        num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(\"Number of learnable parameters for the CNN:\", num_params)\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        # Perform a forward pass through the CNN layers\n",
    "        cnn_output = self.first_cnn_layer(observations)\n",
    "        cnn_output = self.second_cnn_layer(cnn_output)\n",
    "        for index, layer in enumerate(self.cnn_layers):\n",
    "            layer_output = layer(cnn_output)\n",
    "            cnn_output = cnn_output + layer_output\n",
    "        # Flatten the output\n",
    "        cnn_output = self.flatten(cnn_output)\n",
    "        # Pass the flattened output through the linear layer\n",
    "        return self.linear(cnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2c60cd7-9cf6-4f83-bf40-84231a1cc7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 512, position: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        if position is None:\n",
    "            position = torch.arange(max_len)\n",
    "\n",
    "        pe = torch.zeros(1, max_len, d_model)  # Shape: [1, max_len, d_model]\n",
    "            \n",
    "        position = position.float().unsqueeze(1)  # Shape: [1, max_len]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)).unsqueeze(0) \n",
    "        argument = position @ div_term\n",
    "\n",
    "        pe[:, :, 0::2] = torch.sin(argument)\n",
    "        pe[:, :, 1::2] = torch.cos(argument)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        # Instead of adding positional encodings along the sequence dimension,\n",
    "        # we add them along the batch dimension.\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66a20fc-8933-4e3d-9afb-aadbf9fbf853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_integers(integers):\n",
    "    # Repeat each integer three times\n",
    "    repeated_integers = torch.repeat_interleave(integers, 3)\n",
    "    # Trim the tensor to the maximum length\n",
    "    repeated_integers = repeated_integers\n",
    "    return repeated_integers\n",
    "\n",
    "def set_elements_to_zero(row, index):\n",
    "    # Create a mask tensor\n",
    "    mask = torch.zeros_like(row)\n",
    "    mask[index:] = 1\n",
    "    # Set elements before the index to zero\n",
    "    row = row * mask\n",
    "    return torch.nan_to_num(row, neginf = -float('inf'))\n",
    "\n",
    "def step_masking(step_len):\n",
    "    seq_len = step_len * 3\n",
    "    attention_mask = torch.full((seq_len, seq_len), -float('inf'), dtype=torch.float32)\n",
    "    for step in range(1, step_len + 1):\n",
    "        row_set = step - 1\n",
    "        for sequence_element in [row_set * 3, row_set * 3 + 1, row_set * 3 + 2]:\n",
    "            attention_mask[sequence_element] = set_elements_to_zero(attention_mask[sequence_element], step * 3)\n",
    "    \n",
    "    return attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a2bae77-146a-4bdd-ba85-d350c20ac32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerArchitecture(nn.Module):\n",
    "    def __init__(self, d_model=256, n_head=8, n_layer=4, \n",
    "                 d_ff=1024, max_step_len=512, dropout = 0.1, batch_first = True):\n",
    "        super(TransformerArchitecture, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        positions = repeat_integers(torch.arange(0, max_step_len))\n",
    "        \n",
    "        self.positional_embedding = PositionalEncoding(d_model,max_len = max_step_len*3, position = positions)\n",
    "        self.attentions = nn.ModuleList([\n",
    "            nn.MultiheadAttention(d_model, n_head, batch_first = batch_first)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.linear1 = nn.ModuleList([\n",
    "            nn.Linear(d_model, d_ff)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.linear2 = nn.ModuleList([\n",
    "            nn.Linear(d_ff, d_model)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        self.layer_norms1 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_layer)])\n",
    "        self.layer_norms2 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_layer)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Each step is made by reward, observation, action\n",
    "        step_len = x.size(1) // 3 \n",
    "        \n",
    "        x = self.positional_embedding(x)\n",
    "        \n",
    "        # Masking\n",
    "        attention_mask = step_masking(step_len).to(device=x.device, dtype=torch.float32)\n",
    "            \n",
    "        for i in range(len(self.attentions)):\n",
    "            # Multi-head Attention\n",
    "            residual = x\n",
    "            x, _ = self.attentions[i](x, x, x, attn_mask=attention_mask)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = residual + x  # Residual connection\n",
    "            x = self.layer_norms1[i](x)\n",
    "            \n",
    "            # Feed-forward network\n",
    "            residual = x\n",
    "            x = self.activation(self.linear1[i](x))\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.linear2[i](x)\n",
    "            x = residual + x  # Residual connection\n",
    "            x = self.layer_norms2[i](x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a62c7d-fdf8-4956-9212-f5074525fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_embedding_sequence(reward, observation, action):\n",
    "    batch_size, seq_len, emb_dim = reward.size()\n",
    "    combined_tensor = torch.stack((reward, observation, action), dim=1)\n",
    "    #print(combined_tensor.shape)\n",
    "    combined_tensor = combined_tensor.view(batch_size, seq_len * 3, emb_dim)\n",
    "    #print(combined_tensor.shape)\n",
    "    return combined_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "519cf7be-4a66-45b4-9508-6ca01bf36b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTransformers(pl.LightningModule):\n",
    "    def __init__(self, d_model, action_space_dim, observation_space, batch_first = True, max_seq_len = 32):\n",
    "        super(DecisionTransformers, self).__init__()\n",
    "\n",
    "        #reward, action, observation to embedding\n",
    "        self.embedding_reward = nn.Linear(1, d_model)\n",
    "        self.embedding_action = nn.Linear(action_space_dim, d_model)\n",
    "        self.embedding_observation = CustomResNet(observation_space, features_dim = d_model)\n",
    "\n",
    "        #Defining the Transformer architecture\n",
    "        self.emb_dim = d_model\n",
    "        self.transformer = TransformerArchitecture(d_model = d_model, max_step_len=max_seq_len*3, batch_first = batch_first)\n",
    "\n",
    "        #Defining fully connected layer\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(d_model, 2 * d_model),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.output = nn.Linear(2 * d_model,  action_space_dim)\n",
    "\n",
    "        self.huber_loss = nn.SmoothL1Loss()  # Huber loss\n",
    "\n",
    "        # Print the number of learnable parameters\n",
    "        num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(\"Number of learnable parameters for the entire architecture:\", num_params)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        rewards = x[\"rewards\"]\n",
    "        observations = x[\"observations\"]\n",
    "        actions = x[\"actions\"]\n",
    "\n",
    "        #calculating variable needed after\n",
    "        batch_len = observations.shape[0]\n",
    "        seq_len = observations.shape[1]\n",
    "        device = observations.device\n",
    "\n",
    "        #Calculating embedding\n",
    "        rewards_emb = self.embedding_reward(rewards)\n",
    "        actions_emb = self.embedding_action(actions)\n",
    "        observations_emb = torch.empty((batch_len,seq_len, self.emb_dim))\n",
    "        \n",
    "        for batch_index, batch_imgs in enumerate(observations):\n",
    "            observations_emb[batch_index] = self.embedding_observation(batch_imgs)\n",
    "\n",
    "        observations_emb = observations_emb.to(device)\n",
    "        \n",
    "        #interleave the sequences\n",
    "        sequence = merge_embedding_sequence(rewards_emb, observations_emb, actions_emb)\n",
    "\n",
    "        output = self.transformer(sequence)\n",
    "\n",
    "        # Extract the output related to the observation input\n",
    "        sequence = sequence[:, 1::3, :]\n",
    "\n",
    "        #Fully connected layer to get the action\n",
    "        output = self.fc1(output)\n",
    "        \n",
    "        return self.output(output)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        # Compute three different dimension of action space\n",
    "        loss1 = F.huber_loss(y_hat[:, 0], y[:, 0])\n",
    "        loss2 = F.huber_loss(y_hat[:, 1], y[:, 1])\n",
    "        loss3 = F.huber_loss(y_hat[:, 2], y[:, 2])\n",
    "        \n",
    "        # Total loss is the sum of the three losses\n",
    "        total_loss = loss1 + loss2 + loss3\n",
    "        \n",
    "        self.log('train_loss', total_loss)\n",
    "        return total_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        # Compute three different dimension of action space\n",
    "        loss1 = F.huber_loss(y_hat[:, 0], y[:, 0])  # Loss for first value\n",
    "        loss2 = F.huber_loss(y_hat[:, 1], y[:, 1])  # Loss for second value\n",
    "        loss3 = F.huber_loss(y_hat[:, 2], y[:, 2])  # Loss for third value\n",
    "        \n",
    "        # Total loss is the sum of the three losses\n",
    "        total_loss = loss1 + loss2 + loss3\n",
    "        \n",
    "        self.log('val_loss', total_loss)\n",
    "        return total_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2406272-d763-4e6f-bb02-19387689a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceExtractor(Dataset):\n",
    "    def __init__(self, env, seq_len = 32, dataset_len = 16384):\n",
    "        self.seq_len = seq_len\n",
    "        self.dataset_len = dataset_len\n",
    "        self.env = env\n",
    "        self.env_name = env.unwrapped.envs[0].spec.id\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "\n",
    "    ## il seed Ã¨ usato come indice\n",
    "    def __getitem__(self, seed):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        # List all subfolders in the folder\n",
    "        models_subfolder = [f.path for f in os.scandir(self.env_name) if f.is_dir()]\n",
    "        \n",
    "        # Choose a random subfolder\n",
    "        random_model_subfolder = random.choice(models_subfolder)\n",
    "        \n",
    "        files = [f.path for f in os.scandir(random_model_subfolder) if f.is_file()]\n",
    "        assert len(files) > 0, f\"The number of file in the folder {random_model_subfolder} should be greater 0)\"\n",
    "        random_file = random.choice(files)\n",
    "\n",
    "        # Read the Parquet file into a DataFrame\n",
    "        df = pd.read_parquet(random_file)\n",
    "        df['observation'] = df.apply(lambda row : row[\"observation\"].reshape(self.env.observation_space.shape), axis = 1)\n",
    "\n",
    "        #checking if the sequence is long enought (+1 to be sure to have Y too)\n",
    "        assert len(df) > self.seq_len + 1, f\"The lenght of the experience sequence ({x}) should be greater than sequence lenght + 1({y+1})\"\n",
    "        starting_row = random.randint(0, len(df) - (self.seq_len) -1)\n",
    "        ending_row = starting_row + self.seq_len\n",
    "\n",
    "        #converting to numpy array\n",
    "        reward = torch.Tensor(np.stack(df[\"rewards\"][starting_row:ending_row])).unsqueeze(1)\n",
    "        observation = torch.Tensor(np.stack(df[\"observation\"][starting_row:ending_row])).permute(0, 3, 1, 2)\n",
    "        input_action = torch.Tensor(np.stack(df[\"action\"][starting_row:ending_row]))\n",
    "\n",
    "        #print(rewards.shape, observation.shape, input_action.shape)\n",
    "\n",
    "        X = {\n",
    "            \"reward\":reward, \n",
    "            \"observation\":observation, \n",
    "            \"action\":input_action,\n",
    "            }\n",
    "        Y = np.stack(df[\"action\"][starting_row + 1:ending_row + 1])\n",
    "\n",
    "        #print(Y.shape)\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1414e1d7-af78-43bd-80ec-c9abe6e0ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequenceExtractor = SequenceExtractor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e4f7594-dc5e-4abf-ab49-ddf35aa18500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "       data: dict for the X and Y\n",
    "    \"\"\"\n",
    "    Xs, Ys = zip(*data)\n",
    "    \n",
    "    rewards = torch.zeros((len(data),) +  Xs[0][\"reward\"].shape)\n",
    "    observations = torch.zeros((len(data),) +  Xs[0][\"observation\"].shape)\n",
    "    actions = torch.zeros((len(data),) +  Xs[0][\"action\"].shape)\n",
    "    \n",
    "    labels = torch.tensor(np.array(Ys))\n",
    "\n",
    "    #print(labels.shape)\n",
    "    #print(rewards.shape)\n",
    "    #print(observations.shape)\n",
    "    #print(actions.shape)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        rewards[i] = Xs[i][\"reward\"]\n",
    "        observations[i] = Xs[i][\"observation\"]\n",
    "        actions[i] = Xs[i][\"action\"]\n",
    "\n",
    "    return {\n",
    "             \"rewards\": rewards,\n",
    "             \"observations\": observations,\n",
    "             \"actions\":actions,\n",
    "            }, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d409760d-5867-4673-88dc-e99f4c9078f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator().manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01428bac-6c5d-4a4c-b079-10fc903dc306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data loader\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(sequenceExtractor, batch_size=batch_size, \n",
    "                        shuffle=True, num_workers=7, collate_fn = collate_fn,\n",
    "                        worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "556061e1-ae04-41e8-aa07-1663e3a4b37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of learnable parameters for the CNN: 2728064\n",
      "Number of learnable parameters for the entire architecture: 4082051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/cino/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                  | Type                    | Params\n",
      "------------------------------------------------------------------\n",
      "0 | embedding_reward      | Linear                  | 256   \n",
      "1 | embedding_action      | Linear                  | 512   \n",
      "2 | embedding_observation | CustomResNet            | 2.7 M \n",
      "3 | transformer           | TransformerArchitecture | 1.3 M \n",
      "4 | fc1                   | Sequential              | 33.0 K\n",
      "5 | output                | Linear                  | 771   \n",
      "6 | huber_loss            | SmoothL1Loss            | 0     \n",
      "------------------------------------------------------------------\n",
      "4.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 M     Total params\n",
      "16.328    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a4614cfcba405089495f9861a0cf0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/cino/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model = DecisionTransformers(d_model = 128, action_space_dim = env.action_space.shape[0], \n",
    "                             observation_space = env.observation_space, max_seq_len = sequenceExtractor.seq_len)  # Example vocab size\n",
    "trainer = pl.Trainer(max_epochs=10)  # Example trainer configuration\n",
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05452bdc-1d8e-48ec-a75d-62c06cf6c4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
